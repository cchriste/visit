import math

###############################################################################
# Class: JobSubmitter_aprun_NERSC
#
# Purpose:    Custom "aprun" job submitter for NERSC.
#
# Programmer: Brad Whitlock
# Date:       Thu May 17 14:22:04 PDT 2012
#
# Modifications:
#
###############################################################################

class JobSubmitter_aprun_NERSC(JobSubmitter_aprun):
    def __init__(self, launcher):
        super(JobSubmitter_aprun_NERSC, self).__init__(launcher)

    #
    # Override the name of the aprun executable
    #
    def Executable(self):
        return ["env", "DISPLAY=", "CRAY_ROOTFS=DSL", "aprun"]

###############################################################################
# Class: JobSubmitter_qsub_NERSC
#
# Purpose:    Custom "qsub" job submitter for NERSC.
#
# Programmer: Brad Whitlock
# Date:       Thu May 17 14:22:04 PDT 2012
#
# Modifications:
#
###############################################################################

class JobSubmitter_qsub_NERSC(JobSubmitter_qsub):
    def __init__(self, launcher):
        super(JobSubmitter_qsub_NERSC, self).__init__(launcher)

    def CreateFilename(self, root):
        if self.launcher.IsRunningOnHopper():
            tdate = time.asctime()[11:19]
            tuser = self.launcher.username()
            return os.path.join(GETENV("HOME"), "%s.%s.%s" % (root, tuser, tdate))
        return super(JobSubmitter_qsub_NERSC, self).CreateFilename(root)

    def TFileSetup(self, tfile):
        tfile.write("cd %s\n" % os.path.abspath(os.curdir))
        tfile.write("ulimit -c 0\n")
        if self.launcher.IsRunningOnHopper():
            tfile.write("MOM_HOST=`hostname`\n")
            relay = os.path.join(self.launcher.visitbindir, "visit_socket_relay")
            cmd = relay + "%s %s > %s.port\n" % (self.launcher.loginnodehost, self.launcher.loginnodeport, self.tfilename)
            tfile.write(cmd)
            tfile.write("MOM_PORT=`cat %s.port`\n" % self.tfilename)
            tfile.write("rm %s.port\n" % self.tfilename)
            tfile.write("eval $(modulecmd sh unload xt-shmem)\n")
            tfile.write("export CRAY_ROOTFS=DSL\n")

    def SetupPPN(self, nodes, procs, ppn, use_vis):
        if self.launcher.IsRunningOnHopper():
            args = []
            if self.parallel.workingdir != None:
                args = args + ["-d", self.parallel.workingdir]
                if self.parallel.nn != None:
                    nprocpernode = 24
                    nwidth = nprocpernode * int(nodes)
                    # If the number of nodes is set, we need to modify mppwidth to ensure
                    # allocation of the proper number of nodes. The number of nodes allocated
                    # is mppwidth divided by the number of cores per node (compare
                    # http://www.nersc.gov/users/computational-systems/hopper/running-jobs/batch-jobs/).
                    args = args + ["-l", "mppwidth=%s" % str(nwidth)]
                else:
                    # Number of nodes is not specified. We use all cores on each node and do not
                    # need to modify mppdith to ensure the allocation of the appropriate number of
                    # nodes.
                    args = args + ["-l", "mppwidth=%s" % procs]
        elif self.launcher.IsRunningOnCarver():
            new_nodes = nodes
            new_ppn = ppn
            if self.parallel.partition == "reg_xlmem":
                # There is a limit of one node for reg_xlmem
                new_nodes = 1
                new_ppn = procs
            else:
                # Even though Carver nodes may have a different number
                # of cores per nodes, this is a reasonable default for most
                # nodes. It is better to set nodes and cores manually in the
                # launch profile.
                if nodes == None:
                    new_nodes = math.ceil(procs / 8)
                    new_ppn = 8
            args = super(JobSubmitter_qsub_NERSC, self).SetupPPN(new_nodes, procs, new_ppn, use_vis)
        else:
            args = super(JobSubmitter_qsub_NERSC, self).SetupPPN(nodes, procs, ppn, use_vis)
        return args

###############################################################################
# Class: NERSCLauncher
#
# Purpose:    Custom launcher for NERSC
#
# Programmer: Brad Whitlock
# Date:       Thu May 17 14:22:04 PDT 2012
#
# Modifications:
#
###############################################################################

class NERSCLauncher(MainLauncher):
    def __init__(self):
        super(NERSCLauncher, self).__init__()
        self.loginnodeport = None
        self.loginnodehost = None
        self.euclid = -1
        self.hopper = -1
        self.carver = -1

    def IsRunningOnEuclid(self):
        if self.euclid == -1:
            if self.hostname() == "euclid.nersc.gov":
                self.euclid = 1
        return self.euclid

    def IsRunningOnHopper(self):
        if self.hopper == -1:
            self.hopper = GETENV("NERSC_HOST") == "hopper"
        return self.hopper

    def IsRunningOnCarver(self):
        if self.carver == -1:
            if self.sectorname() == "cvrsvc":
                self.carver = 1
        return self.carver

    def Customize(self):
        # Set additional xterm arguments.
        if os.path.exists("/usr/local/anag"):
             self.xterm = ["xterm", "-iconic"]

        # ----
        # Carver @ NERSC
        #
        # ----
        if self.IsRunningOnCarver():
            path = self.splitpaths(GETENV("PATH"))
            addedpaths = ["/usr/syscom/opt/torque/default/bin",
                          "/usr/common/usg/openmpi/1.4.5/gcc/bin"]
            SETENV("PATH", self.joinpaths(addedpaths + path))

        # ----
        # Euclid @ NERSC
        #
        # ----
        if self.IsRunningOnEuclid():
            path = self.splitpaths(GETENV("PATH"))
            addedpaths = ["/usr/common/usg/openmpi/1.4.4/gnu/bin"]
            SETENV("PATH", self.joinpaths(addedpaths + path))

        # ----
        # Hopper @ NERSC
        #
        # ----
        if self.IsRunningOnHopper():
            SETENV("LD_LIBRARY_PATH", "/opt/gcc/4.6.2/snos/lib64")

            # Are we running on a login node?
            if self.sectorname() == "hopper":
                if not self.generalArgs.env and self.generalArgs.exe_name == "gui":
                    msg = """
Do not run the VisIt GUI on Hopper. Run it on you local workstation (preferred/best performance) or Euclid!

For more information about running VisIt at NERSC: http://www.nersc.gov/nusers/resources/software/apps/visualization/visit/

"""
                    exit(msg, 0)

                if self.parallelArgs.parallel:
                    self.loginnodeport = self.generalArgs.port
                    self.generalArgs.port = "$MOM_PORT"
                    self.loginnodehost = self.generalArgs.host
                    self.generalArgs.host = "$MOM_HOST"
                else:
                    exit("The VisIt install on Hopper does not support running a serial engine!", 1)

    #
    # Override the JobSubmitterFactory method so the custom job submitter can
    # be returned.
    #
    def JobSubmitterFactory(self, launch):
        if launch == "aprun":
            return JobSubmitter_aprun_NERSC(self)
        elif launch[:4] == "qsub" or launch[:4] == "msub":
            return JobSubmitter_qsub_NERSC(self)
        return super(NERSCLauncher, self).JobSubmitterFactory(launch)

# Launcher creation function
def createlauncher():
    return NERSCLauncher()
